/**
 * 
 */
package com.zaloni.idc.driver;

import java.io.IOException;
import java.util.Calendar;
import java.util.List;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hive.conf.HiveConf;
import org.apache.hadoop.hive.metastore.HiveMetaStoreClient;
import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;
import org.apache.hadoop.hive.metastore.api.InvalidObjectException;
import org.apache.hadoop.hive.metastore.api.MetaException;
import org.apache.hadoop.hive.metastore.api.Partition;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.MultipleInputs;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.util.Tool;
import org.apache.hadoop.util.ToolRunner;
import org.apache.thrift.TException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import com.zaloni.cdc.exceptions.PartitionNotExistsException;
import com.zaloni.idc.mapper.DeltaFileMapper;
import com.zaloni.idc.mapper.HiveTableMapper;
import com.zaloni.idc.record.IDCRecord;
import com.zaloni.idc.reducer.IDCReducer;

/**
 * @author Revanth
 * 
 */
public class IDCJob extends Configured implements Tool {

	private static Logger LOG = LoggerFactory.getLogger(IDCJob.class);

	private HiveMetaStoreClient hiveClient;

	private Path outPath = new Path("/user/bedrock/idc_out");

	private Path activePartition = null;

	private Path inActivePartition = null;

	String dbName = "retail_schema";
	String tableName = "retailer";
	String partitionToRead = "status=active";
	String inActivePart = "status=inactive";

	private Path archivePath = new Path("/user/bedrock/archive/old");

	private void setUpHiveClient() throws MetaException {
		HiveConf conf = new HiveConf();
		String thriftUrl = "thrift://10.11.12.245:9083";
		/*
		 * if (null == thriftUrl) { thriftUrl = "thrift:///"; }
		 */
		conf.set("hive.metastore.uris", thriftUrl);
		this.hiveClient = new HiveMetaStoreClient(conf);
	}

	private Path getHiveInputPath(Configuration conf) throws IOException,
			MetaException, TException, PartitionNotExistsException {
		setUpHiveClient();
		List<String> parts = this.hiveClient.listPartitionNames(dbName,
				tableName, (short) 100);
		checkPartitionsExist(parts);
		Partition partition = this.hiveClient.getPartition(dbName, tableName,
				partitionToRead);
		activePartition = new Path(partition.getSd().getLocation());
		this.hiveClient.close();
		return activePartition;
	}

	private void checkPartitionsExist(List<String> parts)
			throws PartitionNotExistsException, InvalidObjectException,
			AlreadyExistsException, MetaException, TException {
		if (!parts.contains(partitionToRead)) {
			throw new PartitionNotExistsException(
					"Active partion does not exist for table " + this.tableName);
		}
		if (!parts.contains(inActivePart)) {
			Partition partition = this.hiveClient.appendPartition(dbName,
					tableName, inActivePart);
			inActivePartition = new Path(partition.getSd().getLocation());
		} else {
			inActivePartition = new Path(this.hiveClient
					.getPartition(dbName, tableName, inActivePart).getSd()
					.getLocation());
		}
	}

	public int run(String[] args) throws Exception {
		Configuration conf = getConf();
		boolean jobCompleted = false;
		conf.set("delimiter", ",");
		conf.set("idcIndex", "5");
		conf.set("dateformat", "MM/dd/yyyy");
		Job job = Job.getInstance(conf, "IDCJob");
		job.setJarByClass(IDCJob.class);

		MultipleInputs.addInputPath(job, new Path("/user/bedrock/delta.txt"),
				TextInputFormat.class, DeltaFileMapper.class);
		MultipleInputs.addInputPath(job, getHiveInputPath(conf),
				TextInputFormat.class, HiveTableMapper.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(IDCRecord.class);
		job.setReducerClass(IDCReducer.class);
		job.setNumReduceTasks(1);
		job.setOutputKeyClass(NullWritable.class);
		job.setOutputValueClass(Text.class);
		FileOutputFormat.setOutputPath(job, outPath);
		MultipleOutputs.addNamedOutput(job, "hive", TextOutputFormat.class,
				Text.class, Text.class);
		MultipleOutputs.addNamedOutput(job, "inactive", TextOutputFormat.class,
				Text.class, Text.class);
		jobCompleted = job.waitForCompletion(true);
		if (jobCompleted) {
			moveOutFilesToTable(conf);
		}
		return jobCompleted ? 0 : 1;
	}

	private void moveOutFilesToTable(Configuration conf)
			throws IllegalArgumentException, IOException {
		LOG.info("IDCjob.moveOutFilesToTable BEGIN ");
		FileSystem fs = FileSystem.get(conf);
		if (!fs.exists(archivePath)) {
			fs.mkdirs(archivePath);
		}
		FileStatus[] activeFiles = fs.globStatus(new Path(outPath.toString()
				+ Path.SEPARATOR + "hive*"));
		FileStatus[] inActiveFiles = fs.globStatus(new Path(outPath.toString()
				+ Path.SEPARATOR + "inactive*"));
		FileStatus[] currentFiles = fs.listStatus(activePartition);
		for (FileStatus inActiveFile : inActiveFiles) {
			System.out.println("moving inactive files ++++ "
					+ inActiveFile.getPath());
			String fileName = inActiveFile.getPath().getName()
					+ Calendar.getInstance().getTimeInMillis();
			fs.rename(inActiveFile.getPath(),
					new Path(inActivePartition.toString() + Path.SEPARATOR
							+ fileName));
		}
		for (FileStatus currentFile : currentFiles) {
			fs.rename(currentFile.getPath(),
					new Path(archivePath.toString() + Path.SEPARATOR
							+ Calendar.getInstance().getTimeInMillis()));
		}
		for (FileStatus activeFile : activeFiles) {
			fs.rename(activeFile.getPath(), new Path(activePartition.toString()
					+ Path.SEPARATOR + "active"
					+ Calendar.getInstance().getTimeInMillis()));
		}
		LOG.info("IDCjob.moveOutFilesToTable END ");
	}

	public static void main(String[] args) {
		try {
			int res = ToolRunner.run(new Configuration(), new IDCJob(), args);
			System.exit(res);
		} catch (Exception e) {
			e.printStackTrace();
		}
	}

}
